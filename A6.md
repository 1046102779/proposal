gRPC Retry Design
----
* Author(s): [Noah Eisen](github.com/ncteisen) and [Eric Gribkoff](https://github.com/ericgribkoff)
* Approver: a11r
* Status: Draft
* Implemented in: All Languages
* Last updated: 02/10/2017
* Discussion at: 

## Abstract

gRPC client library will automatically retry failed RPCs according to a policy set by the service owner.

## Background

Currently, gRPC does not retry failed RPCs. All failed RPCs are immediately returned to the application layer by the gRPC client library.

Many teams have implemented their own retry logic wrapped around gRPC like [Veneer Toolkit](https://github.com/googleapis/toolkit) and [Cloud Bigtable](https://github.com/GoogleCloudPlatform/cloud-bigtable-client).

## Proposal

### Overview

gRPC will support two configurable retry policies. The service configuration may choose from a retry policy (retry failed RPCs) or a hedging policy (aggressively send the same RPC multiple times in parallel). An individual RPC may be governed by a retry policy or a hedge policy, but not both.

Retry policy capabilities are as follows. Each has a detailed description below.
* Maximum number of retry attempts
* Exponential backoff
* Set of retryable status codes

The hedging policy has the following parameters. See details here.
* Maximum number of hedged requests
* Delay between hedged requests
* Set of non-fatal status codes

Additionally, gRPC provides a mechanism to throttle retry attempts and hedged RPCs when the ratio of failures to successes exceeds a threshold. See detailed description of throttling.

We also provide a mechanism for servers to explicitly signal clients to retry after a settable delay. See detailed description of server pushback.

In some cases, gRPC can guarantee that a request has never been seen by the server application logic. These cases will be transparently retried by gRPC, as detailed here.

Lastly, information about number of retry attempts will be exposed to the client and server applications through metadata. Find more details here.

### Detailed Design

#### Retry Policy Capabilities

Retry policies support configuring the maximum number of retries, the parameters for exponential backoff, and the set of retryable status codes. Each of these configuration options is detailed in its own section below.

##### Maximum Number of Retries

The maximum number of retry attempts per original RPC. This can be specified in the retry policy as follows:

```
'max_retry_attempts': 3
```

gRPC's call deadlines apply to the entire chain of original RPC and retry attempts. The time limit begins when the original RPC is issued to the gRPC client library. For example, if the specified deadline is 500ms and after 500ms only one retry has been attempted, the call is cancelled even if max_retry_attempts is set to two or greater.

##### Exponential Backoff

The retry policy may specify the following parameters for exponential backoff:

```
'exponential_backoff' {
  'initial_backoff_ms': 1000,
  'max_backoff_ms': 5000,
  'multiplier': 3
}
```

A value called current_backoff is initially set to initial_backoff. After every failed RPC, current_backoff is set to min(current_backoff * multiplier, max_backoff). The failed RPCs will be retried after x seconds, where x is defined as random(0, current_backoff).

##### Retryable Status Codes

When gRPC receives a non-OK response status from a server, this status is checked against the set of retryable status codes to determine if a retry attempt should be made.

```
'retryable_status_codes': {UNAVAILABLE}
```

In general, only status codes that indicate the service did not process the request should be retried. However, in the case of idempotent methods, a more aggressive set of parameters can be specified, as there is no harm in the server processing the request more than once.

#### Hedging Policy

Hedging enables aggressively sending multiple copies of a single request without waiting for a response. Because hedged RPCs may be be executed multiple times on the server side, typically by different backends, it is important that hedging is only enabled for methods that are safe to execute multiple times without adverse affect.

Hedged requests are configured with the following parameters:

```
'hedging_policy': {
  'max_requests' : 3,
  'hedging_delay_ms' : 500,
  'non_fatal_status_codes': {UNAVAILABLE, INTERNAL, ABORTED}
}
```

When a method has hedging_params set, the first RPC is sent immediately, as with a standard non-hedged call. After hedging_delay has elapsed without a successful response, the second RPC will be issued. If neither RPC has received a response after hedging_delay has elapsed again, a third RPC is sent, and so on, up to max_requests. In the above configuration, after 1ms there would be one outstanding hedged RPC, after 501ms there would be two outstanding hedged RPCs, and after 1001ms there would be three outstanding hedged RPCs.

The implementation will ensure that the listener returned to the client application forwards its calls (such as onNext or onClose) to all outstanding hedged RPCs.

When a non-error response is received (in response to any of the hedged requests), all outstanding hedged requests are canceled and the response is returned to the client application layer.

If a non-fatal status code is received from a hedged request, then the next hedged request in line is sent immediately, shortcutting its hedging delay. If any other status code is received, all outstanding RPCs are canceled and the error is returned to the client application layer.

If all instances of a hedged RPC fail, there are no additional retry attempts. Essentially, hedging can be seen as retrying the original RPC before a failure is even received. 

If server pushback that specifies not to retry is received in response to a hedged request, no further hedged requests should be issued for the call.

Hedged requests should be sent to distinct backends, if possible.

#### Throttling Retry Attempts and Hedged RPCs

gRPC prevents server overload due to retries and hedged RPCs by disabling these policies when the client’s ratio of failures to successes passes a certain threshold. The throttling is done per server name. Retry throttling may be configured as follows:

```
'retry_throttling': {
  'max_tokens': 10,
  'token_ratio': 0.1
}
```

Throttling may only be specified per server name, rather than per method or per service.

For each server name, gRPC maintains a token_count which is initially set to max_tokens. Every outgoing RPC (regardless of service or method invoked) will effect token_count as follows:

* Every failed RPC will decrement the token_count by 1. 
* Every successful RPC will increment the token_count by token_ratio. 

If token_count is less than or equal to the threshold, defined to be (max_tokens / 2), then RPCs will not be retried until token_count rises over the threshold.

Throttling also applies to hedged RPCs. The first outgoing RPC will always be sent, but subsequent hedged RPCs will only be sent if token_count is greater than the threshold.

Neither retry attempts nor hedged RPCs block when token_count is less than or equal to the threshold. Retry attempts are canceled and the failure returned to the client application. The hedged request is cancelled, and if there are no other already-sent hedged RPCs the failure is returned to the client application.

The only RPCs that are counted as failures for the throttling policy are RPCs that fail with a status code that qualifies as a retriable or non-fatal status code (see here and here), or that receive a pushback response indicating not to retry. This avoids conflating server failure with responses to malformed requests (such as the INVALID_ARGUMENT status code).

#### Pushback
Servers may explicitly pushback by setting metadata in their response to the client. The pushback can either tell the client to retry after a given delay or to not retry at all. If the client has already exhausted its max_retry_attempts, the call will not be retried even if the server says to retry after a given delay.

A new metadata key, “x-grpc-retry-pushback-ms”, will be added to support server pushback. If the value for pushback is set to -1, then it will be seen as the server telling the client not to retry at all.

#### Summary of Retry and Hedging Logic
There are five possible types of server responses. The list below enumerates the behavior of retry and hedging policies for each type of response. In all cases, if the maximum number of retry attempts or the maximum number of hedged requests is reached, no further RPCs are sent. Hedged RPCs are returned to the client application when all outstanding and pending requests have either received a response or been canceled.

1. OK
  1. Retry policy: Successful response, return success to client application
  2. Hedging policy: Successful response, cancel previous and pending hedges
2. Retryable/Non-Fatal Status Code
  1. Retry policy: Retry according to policy
  2. Hedging policy: Immediately send next scheduled hedged request, if any. Subsequent hedged requests will resume at hedging_delay
3. Fatal Status Code
  1. Retry policy: Don't retry, return failure to client application
  2. Hedging policy: Cancel previous and pending hedges
4. Pushback: Don't retry
  1. Retry policy: Don't retry, return failure to client application
  2. Hedging policy: Don’t send any more hedged requests.
5. Pushback: Retry in n ms
  1. Retry policy: Retry in n ms. If this attempt also fails, retry delay will reset to initial backoff for the following retry (if applicable)
  2. Hedging policy: Send next hedged request in n ms. Subsequent hedged requests will resume at n + hedging_delay

![State Diagram](A6_graphics/StateDiagram.png)

### Retry Internals

#### Where Retries Occur

The retry policy will be implemented in-between the channel and the load balancing policy. That way every retry gets a chance to be sent out on a different subchannel than it originally failed on.

![Where Retries Occur](A6_graphics/WhereRetriesOccur.png)

#### When Retries are Valid

In certain cases it is not valid to retry an RPC. These cases occur when the RPC has been committed, and thus it does not make sense to perform the retry.

An RPC becomes committed in two scenarios:

1. The client receives a non-error response (either an explicit OK status or any response message) from the server.
2. The client’s outgoing message has overflowed the gRPC client library’s buffer.

The reasoning behind the first scenario is that as soon as a non-error response is seen from the server, it is transmitted up to the client application. This may fundamentally change the state of the client, so we cannot safely retry if a failure occurs later in the RPC’s life.

To clarify the second scenario, we define an outgoing message as everything the client sends on its connection to the server. For unary and server streaming calls, the outgoing message is a single message. For client and bidirectional streaming calls, the outgoing message is the entire message stream issued by the client after opening the connection. The gRPC client library buffers outgoing messages, and as long as the entirety of the outgoing message is in the buffer, it can be resent and retried. But as soon as the outgoing message grows too large to buffer, the gRPC client library cannot replay the entire stream of messages, and thus retries are not valid.

#### Buffered RPCs

The gRPC library will have a configurable amount of available memory (retry_buffer_size), to buffer outgoing retryable or hedged RPCs. There is also a per-RPC size limit (per_rpc_buffer_limit). These limits are configured by the client, rather than coming from the service config.

RPCs may only be retried when they are contained in the buffer. 

Hedged RPCs with a non-zero delay will only send out the subsequent hedged requests when the original RPC remains in the buffer. For hedge policies with a delay of zero milliseconds, the RPC does not need to be placed in the buffer at all and the outgoing RPCs can be sent immediately then discarded. 

After the RPC response has been returned to the client application layer, the RPC is removed from the buffer.

New RPCs which do not fit in the available buffer space (either due to the total available buffer space, or due to the per-RPC limit) will not be retryable. For client-side streaming RPCs, if additional outgoing messages would cause the buffered size to exceed the per-RPC limit, the entire message will be removed from the buffer. 

When an RPC is evicted from the buffer, pending hedged requests should be canceled immediately. Implementations must avoid potential scheduling race conditions when canceling pending requests concurrently with incoming responses to already sent hedges, and ensure that failures are relayed to the client application logic when no more hedged requests will be possible and all outstanding requests have returned.

#### Transparent Retries

RPC failures can occur in three distinct ways:

1. The RPC never leaves the client.
2. The RPC reaches the server, but has never been seen by the server application logic.
3. The RPC is seen by the server application logic, and fails.

![Where RPCs Fail](A6_graphics/WhereRPCsFail.png)

The last case is handled by the configurable retry policy that is the main focus of this document. The first two cases are retried automatically by the gRPC client library, regardless of the retry configuration set by the service owner. We are able to do this because these request have not made it to the server application logic, and thus are always safe to retry.

In the first case, in which the RPC never leaves the client, the client library will immediately retry the call once. If this immediate retry fails, then the failure will be treated just like the third bullet point. This means that it will be handled by the configured retry policy.

Similarly, if the RPC reaches the server, but has never been seen by the server application logic (the second bullet), the client library will immediately retry it once. If this fails, then the RPC will be handled by the configured retry policy.

Since retry throttling is designed to prevent server application overload, and these transparent retries do not make it to the server application layer, they do not count as failures when deciding whether to throttle retry attempts.

#### Exposed Retry Metadata
Both client and server application logic will have access to data about retries via gRPC metadata. Upon seeing an RPC from the client, the server will know if it was a retry, and moreover, it will know the number of previously made attempts. Likewise, the client will receive the number of retry attempts made when receiving the results of an RPC.

The new header names for exposing the metadata will be "x-grpc-retry-attempts" to give clients and servers access to the attempt count.

#### Disabling Retries

Clients cannot override retry policy set by the service config. However, retry support can be disabled entirely within the gRPC client library. This is designed to enable existing libraries that wrap gRPC with their own retry implementation (such as Veneer Toolkit) to avoid having retries taking place at the gRPC layer and within their own libraries.

Eventually, retry logic should be taken out of the wrapping libraries, and only exist in gRPC. But allowing the retries to be disabled entirely will make that rollout process easier.

#### Retry and Hedging Statistics

gRPC will treat each retry attempt or hedged RPC as a distinct RPC with regards to the current per-RPC metrics. For example, when an RPC fails with a retryable status code and a retry attempt is made, the original request and the retry attempt will be recorded as two separate RPCs.

Additionally, to present a clearer picture of retry attempts, we add three additional per-method metrics:

1. Total number of retry attempts made
2. Total number of retry attempts which failed
3. A histogram of retry attempts made:
  1. The number of retry attempts will be classified into the following buckets: 
    1. >=1, >=2, >=3, >=4, >=5, >=10, >=100, >=1000
  2. Each retry attempt increments the count in exactly bucket. For example:
    1. The 1st retry attempt adds one to the ">=1" count
    2. The 2nd retry attempt adds one to the ">=2" count, leaving the count for ">=1" unchanged.
    3. The 5th through 9th retry attempts each add one to the ">=5" count.
    4. The 10th through 99th retry attempts each add one to the ">=10" count.

For hedged requests, we record the same stats as above, treating the first hedged request as the initial RPC and subsequent hedged requests as retry attempts.

### Configuration Language

Retry and hedging configuration is set as part of the service config, which is transmitted to the client during DNS resolution. Like other aspects of the service config, retry or hedging policies can be specified per-method, per-service, or per-server name.

Service owners must choose between a retry policy or a hedging policy. Unless the service owner specifies a policy in the configuration, retries and hedging will not be enabled. The retry policy and hedging policy each have their own set of configuration options, detailed below. 

The parameters for throttling retry attempts and hedged RPCs when failures exceed a certain threshold are also set in the service config. Throttling applies across methods and services on a particular server, and thus may only be configured per-server name.

#### Retry Policy

This is an example of a retry policy and its associated configuration. It implements exponential backoff with a maximum of three retry attempts, only retrying RPCs when an UNAVAILABLE status code is received.

```
'retry_policy': {
  'max_retry_attempts': 3
  'exponential_backoff': {
    'initial_backoff_ms': 1000,
    'max_backoff_ms': 5000,
    'multiplier': 3
  }
  'retryable_status_codes': {UNAVAILABLE}
}
```

#### Hedging Policy

The following example of a hedging policy configuration will issue up to three hedged requests for each RPC, spaced out at 500ms intervals, until either: one of the requests receives a valid response, all fail, or the overall call deadline is reached. Analogously to retryable_status_codes for the retry policy, non_fatal_status_codes determines how hedging behaves when a non-OK response is received.

```
'hedging_policy': { 
  'max_requests': 3, 
  'hedging_delay_ms': 500,
  'non_fatal_status_codes': {UNAVAILABLE, INTERNAL, ABORTED}
}
```

The following example issues three hedged requests simultaneously:

```
'hedging_policy': { 
  'max_requests': 3,
  'hedging_delay_ms': 0,
  'non_fatal_status_codes': {UNAVAILABLE, INTERNAL, ABORTED}
}
```

#### Throttling Configuration
Throttling configuration applies to all services and methods on a given server, and so can only be set per-server name. The following configuration throttles retry attempts and hedged RPCs when the ratio of failures to successes exceeds ~10%.

```
'retry_throttling': {
    'max_tokens': 10,
    'token_ratio': 0.1
}
```

## Open issues

### Security

Discuss the DNS issue here
